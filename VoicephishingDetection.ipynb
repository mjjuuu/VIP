{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KctjwfrZTsw_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2805d9f7-97ca-48f2-9e90-749b5b6a0e2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# êµ¬ê¸€ë“œë¼ì´ë¸Œì—°ë™\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì‘ì—…ë””ë ‰í† ë¦¬ ì„¤ì • (voicephishing1í´ë”)\n",
        "import os\n",
        "\n",
        "# ì‘ì—…í•  ë””ë ‰í† ë¦¬ ì„¤ì •\n",
        "work_dir = '/content/drive/My Drive/voicephishingDetection'\n",
        "\n",
        "# ì‘ì—… ë””ë ‰í† ë¦¬ë¡œ ì´ë™\n",
        "os.chdir(work_dir)\n",
        "\n",
        "# í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ í™•ì¸\n",
        "print(\"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬:\", os.getcwd())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLu1r2rTVeYd",
        "outputId": "333b53a3-89b9-4989-8228-239942bb7274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: /content/drive/My Drive/voicephishingDetection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7ì°¨ lstmí•™ìŠµì½”ë“œ -> ì‹œí€€ìŠ¤ 30ìœ¼ë¡œ ì œí•œ train_data.csv , test_data.csv, val_data.csvíŒŒì¼ë¡œ ì‚¬ì „ì— ë‚˜ëˆ”. (labelë°¸ëŸ°ì‹±, call_idê¸°ì¤€ìœ¼ë¡œ ì ìš©)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# -------------------------\n",
        "# 1. í•™ìŠµ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬\n",
        "# -------------------------\n",
        "train_df = pd.read_csv('train_data.csv', encoding='utf-8')\n",
        "\n",
        "train_texts = train_df['transcript'].astype(str).values\n",
        "train_labels = train_df['label'].values\n",
        "\n",
        "# -------------------------\n",
        "# 2. ê²€ì¦ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬\n",
        "# -------------------------\n",
        "valid_df = pd.read_csv('val_data.csv', encoding='utf-8')\n",
        "\n",
        "valid_texts = valid_df['transcript'].astype(str).values\n",
        "valid_labels = valid_df['label'].values\n",
        "\n",
        "# -------------------------\n",
        "# 3. í† í¬ë‚˜ì´ì € ë° ì‹œí€€ìŠ¤ ë³€í™˜ (train ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµ)\n",
        "# -------------------------\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(train_texts), maxlen=30, padding='post', truncating='post')\n",
        "y_train = np.array(train_labels)\n",
        "\n",
        "X_valid = pad_sequences(tokenizer.texts_to_sequences(valid_texts), maxlen=30, padding='post', truncating='post')\n",
        "y_valid = np.array(valid_labels)\n",
        "\n",
        "# -------------------------\n",
        "# 4. LSTM ëª¨ë¸ êµ¬ì„±\n",
        "# -------------------------\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=128, input_length=30))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# -------------------------\n",
        "# 5. ëª¨ë¸ í•™ìŠµ (validation í¬í•¨)\n",
        "# -------------------------\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# -------------------------\n",
        "# 6. tokenizer, ëª¨ë¸ ì €ì¥\n",
        "# -------------------------\n",
        "with open('/content/drive/MyDrive/voicephishingDetection/tokenizer0514.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "model.save('/content/drive/MyDrive/voicephishingDetection/lstm_model0514.keras')\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ ì™„ë£Œ, ì €ì¥ë„ ì™„ë£Œ\")\n",
        "\n",
        "# -------------------------\n",
        "# 7. ê²€ì¦ ë°ì´í„° ì„±ëŠ¥ ì¶œë ¥\n",
        "# -------------------------\n",
        "val_loss, val_acc = model.evaluate(X_valid, y_valid)\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xa4rNAEY79j",
        "outputId": "b1981aa2-19e8-46a8-d042-35d49d33e251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m348/348\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 119ms/step - accuracy: 0.8942 - loss: 0.2282 - val_accuracy: 0.9921 - val_loss: 0.0261\n",
            "Epoch 2/5\n",
            "\u001b[1m348/348\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 115ms/step - accuracy: 0.9941 - loss: 0.0200 - val_accuracy: 0.9903 - val_loss: 0.0246\n",
            "Epoch 3/5\n",
            "\u001b[1m348/348\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 117ms/step - accuracy: 0.9964 - loss: 0.0110 - val_accuracy: 0.9803 - val_loss: 0.0358\n",
            "Epoch 4/5\n",
            "\u001b[1m348/348\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 111ms/step - accuracy: 0.9963 - loss: 0.0114 - val_accuracy: 0.9918 - val_loss: 0.0221\n",
            "Epoch 5/5\n",
            "\u001b[1m348/348\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 108ms/step - accuracy: 0.9984 - loss: 0.0057 - val_accuracy: 0.9907 - val_loss: 0.0227\n",
            "âœ… ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦ ì™„ë£Œ, ì €ì¥ë„ ì™„ë£Œ\n",
            "\u001b[1m88/88\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9889 - loss: 0.0188\n",
            "Validation Accuracy: 0.9907\n",
            "Validation Loss: 0.0227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ê²€ì¦ ë¼ë²¨ ë¶„í¬ í™•ì¸\n",
        "print(valid_df['label'].value_counts())\n",
        "\n",
        "# call_id ê²¹ì¹˜ëŠ”ì§€ í™•ì¸\n",
        "train_ids = set(train_df['call_id'].unique())\n",
        "valid_ids = set(valid_df['call_id'].unique())\n",
        "print(\"ê²¹ì¹˜ëŠ” call_id ìˆ˜:\", len(train_ids & valid_ids))"
      ],
      "metadata": {
        "id": "TJacRRmqY9oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7ì°¨ lstmí•™ìŠµëª¨ë¸ ê²€ì¦ -> test_data.csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# -------------------------------\n",
        "# 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©\n",
        "# -------------------------------\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/voicephishing1/test_data.csv', encoding='utf-8-sig')\n",
        "test_texts = test_df['transcript'].astype(str).values\n",
        "test_labels = test_df['label'].values\n",
        "call_ids = test_df['call_id'].values  # call_id ì¶”ê°€\n",
        "\n",
        "# -------------------------------\n",
        "# 2. ì €ì¥ëœ Tokenizer ë¡œë“œ\n",
        "# -------------------------------\n",
        "with open('/content/drive/MyDrive/voicephishing1/tokenizer0514.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. í…ìŠ¤íŠ¸ â†’ ì‹œí€€ìŠ¤ ë³€í™˜ ë° íŒ¨ë”©\n",
        "# -------------------------------\n",
        "X_test = tokenizer.texts_to_sequences(test_texts)\n",
        "X_test = pad_sequences(X_test, maxlen=30, truncating='post', padding='post')\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\n",
        "# -------------------------------\n",
        "model = load_model('/content/drive/MyDrive/voicephishing1/lstm_model0514.keras')\n",
        "\n",
        "# -------------------------------\n",
        "# 5. ê°œë³„ ë¬¸ì¥ì˜ ì˜ˆì¸¡ê°’ (í™•ë¥ ) êµ¬í•˜ê¸°\n",
        "# -------------------------------\n",
        "predictions = model.predict(X_test)  # ì˜ˆì¸¡ê°’ (0ê³¼ 1 ì‚¬ì´ì˜ í™•ë¥ )\n",
        "\n",
        "# -------------------------------\n",
        "# 6. call_idë³„ë¡œ ì˜ˆì¸¡ í™•ë¥ ì˜ í‰ê· ê°’ ê³„ì‚°\n",
        "# -------------------------------\n",
        "test_df['prediction'] = predictions  # ì˜ˆì¸¡ í™•ë¥ ì„ DataFrameì— ì¶”ê°€\n",
        "\n",
        "# call_idë³„ë¡œ í‰ê·  ì˜ˆì¸¡ í™•ë¥  ê³„ì‚° (ê° í†µí™”ë§ˆë‹¤ í‰ê·  í™•ë¥ )\n",
        "call_id_avg_predictions = test_df.groupby('call_id')['prediction'].mean()\n",
        "\n",
        "# -------------------------------\n",
        "# 7. ê° í†µí™”ì— ëŒ€í•´ ë³´ì´ìŠ¤í”¼ì‹± ì—¬ë¶€ íŒë‹¨\n",
        "# -------------------------------\n",
        "# ì˜ˆ: í‰ê·  í™•ë¥ ì´ 0.5 ì´ìƒì´ë©´ í”¼ì‹± í†µí™”ë¡œ íŒë‹¨\n",
        "call_id_avg_predictions = call_id_avg_predictions.apply(lambda x: 1 if x >= 0.5 else 0)\n",
        "\n",
        "# -------------------------------\n",
        "# 8. ì „ì²´ ì„±ëŠ¥ í‰ê°€ (ì „ì²´ í†µí™” ê¸°ì¤€ìœ¼ë¡œ í‰ê°€)\n",
        "# -------------------------------\n",
        "# í…ŒìŠ¤íŠ¸ì…‹ì˜ ì‹¤ì œ ë¼ë²¨ê³¼ ì˜ˆì¸¡ ë¼ë²¨ì„ ë¹„êµ\n",
        "true_labels = test_df.groupby('call_id')['label'].first()  # í†µí™”ë³„ ì²« ë²ˆì§¸ ë¼ë²¨ë¡œ ì°¸ê°’ ì„ íƒ\n",
        "pred_labels = call_id_avg_predictions  # í†µí™”ë³„ ì˜ˆì¸¡ê°’\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# ì„±ëŠ¥ í‰ê°€ ì§€í‘œ ì¶œë ¥\n",
        "accuracy = accuracy_score(true_labels, pred_labels)\n",
        "precision = precision_score(true_labels, pred_labels)\n",
        "recall = recall_score(true_labels, pred_labels)\n",
        "f1 = f1_score(true_labels, pred_labels)\n",
        "\n",
        "print(f\"âœ… ì •í™•ë„: {accuracy:.4f}\")\n",
        "print(f\"âœ… ì •ë°€ë„: {precision:.4f}\")\n",
        "print(f\"âœ… ì¬í˜„ìœ¨: {recall:.4f}\")\n",
        "print(f\"âœ… F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "4ICJVerCZMIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7ì°¨ lstmí•™ìŠµëª¨ë¸ ê³¼ì í•© ì—¬ë¶€ íŒë‹¨ 1\n",
        "import random\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# ëª¨ë¸ê³¼ Tokenizer ë¡œë”©\n",
        "def load_model_and_tokenizer():\n",
        "    # LSTM ëª¨ë¸ê³¼ Tokenizer ë¡œë“œ\n",
        "    model = tf.keras.models.load_model('/content/drive/MyDrive/voicephishing1/lstm_model0514.keras')\n",
        "    with open('/content/drive/MyDrive/voicephishing1/tokenizer0514.pkl', 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    return model, tokenizer\n",
        "\n",
        "# ê¸°ì¡´ ë°ì´í„° ë³µì‚¬\n",
        "test_df_flipped = test_df.copy()\n",
        "\n",
        "# í†µí™”(call_id) ë‹¨ìœ„ë¡œ ë¼ë²¨ ì¼ë¶€ ë’¤ì§‘ê¸° (ì˜ˆ: 10%)\n",
        "call_ids = test_df_flipped['call_id'].unique()\n",
        "num_to_flip = max(1, int(len(call_ids) * 0.1))\n",
        "flipped_call_ids = random.sample(list(call_ids), num_to_flip)\n",
        "\n",
        "# call_id ë‹¨ìœ„ë¡œ ë¼ë²¨ ë°˜ì „\n",
        "for cid in flipped_call_ids:\n",
        "    original_label = test_df_flipped.loc[test_df_flipped['call_id'] == cid, 'label'].iloc[0]\n",
        "    test_df_flipped.loc[test_df_flipped['call_id'] == cid, 'label'] = 1 - original_label\n",
        "\n",
        "print(f\"ğŸ”„ ë¼ë²¨ ë’¤ì§‘ì€ í†µí™” ìˆ˜: {num_to_flip}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 1. ëª¨ë¸ê³¼ Tokenizer ë¡œë“œ\n",
        "# -------------------------------\n",
        "model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "# -------------------------------\n",
        "# 2. ë¼ë²¨ ë°˜ì „ëœ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ (ê° í†µí™”ë³„ë¡œ)\n",
        "# -------------------------------\n",
        "# test_df_flippedì—ì„œ 'call_id'ë³„ë¡œ ìŒì„± í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì˜ˆì¸¡\n",
        "flipped_texts = test_df_flipped['transcript'].astype(str).values\n",
        "X_flipped = tokenizer.texts_to_sequences(flipped_texts)\n",
        "X_flipped = pad_sequences(X_flipped, maxlen=30, truncating='post', padding='post')\n",
        "\n",
        "# ì˜ˆì¸¡ê°’ (0ê³¼ 1 ì‚¬ì´ì˜ í™•ë¥ )\n",
        "flipped_predictions = model.predict(X_flipped)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. call_idë³„ë¡œ ì˜ˆì¸¡ í™•ë¥ ì˜ í‰ê· ê°’ ê³„ì‚°\n",
        "# -------------------------------\n",
        "test_df_flipped['prediction'] = flipped_predictions  # ì˜ˆì¸¡ í™•ë¥ ì„ DataFrameì— ì¶”ê°€\n",
        "call_id_avg_predictions_flipped = test_df_flipped.groupby('call_id')['prediction'].mean()\n",
        "\n",
        "# -------------------------------\n",
        "# 4. call_idë³„ë¡œ ë¼ë²¨ ë°˜ì „ í›„ ì˜ˆì¸¡ê°’ í‰ê°€\n",
        "# -------------------------------\n",
        "# ë‹¤ì‹œ ê·¸ë£¹í•‘ ë° í‰ê°€\n",
        "true_labels_flipped = test_df_flipped.groupby('call_id')['label'].first()  # í†µí™”ë³„ ì²« ë²ˆì§¸ ë¼ë²¨ë¡œ ì°¸ê°’ ì„ íƒ\n",
        "pred_labels_flipped = call_id_avg_predictions_flipped.apply(lambda x: 1 if x >= 0.5 else 0)  # 0.5 ì´ìƒì´ë©´ í”¼ì‹± í†µí™”ë¡œ íŒë‹¨\n",
        "\n",
        "# ì„±ëŠ¥ í‰ê°€ ì§€í‘œ ì¶œë ¥\n",
        "accuracy = accuracy_score(true_labels_flipped, pred_labels_flipped)\n",
        "precision = precision_score(true_labels_flipped, pred_labels_flipped)\n",
        "recall = recall_score(true_labels_flipped, pred_labels_flipped)\n",
        "f1 = f1_score(true_labels_flipped, pred_labels_flipped)\n",
        "\n",
        "print(\"\\nâœ… [ë¼ë²¨ ë°˜ì „ í…ŒìŠ¤íŠ¸]\")\n",
        "print(f\"ì •í™•ë„: {accuracy:.4f}\")\n",
        "print(f\"ì •ë°€ë„: {precision:.4f}\")\n",
        "print(f\"ì¬í˜„ìœ¨: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "cktjrH6vZPSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7ì°¨ lstmí•™ìŠµëª¨ë¸ ê³¼ì í•© ì—¬ë¶€ íŒë‹¨ 2\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ëª¨ë¸ê³¼ Tokenizer ë¡œë”© í•¨ìˆ˜\n",
        "def load_model_and_tokenizer():\n",
        "    # LSTM ëª¨ë¸ê³¼ Tokenizer ë¡œë“œ\n",
        "    model = tf.keras.models.load_model('/content/drive/MyDrive/voicephishing1/lstm_model0514.keras')\n",
        "    with open('/content/drive/MyDrive/voicephishing1/tokenizer0514.pkl', 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    return model, tokenizer\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ì„ê¸° í•¨ìˆ˜\n",
        "def shuffle_words(text):\n",
        "    words = text.split()\n",
        "    if len(words) > 1:\n",
        "        random.shuffle(words)\n",
        "    return ' '.join(words)\n",
        "\n",
        "# ëª¨ë¸ê³¼ Tokenizer ë¡œë“œ\n",
        "model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³µì‚¬ ë° í…ìŠ¤íŠ¸ ì¼ë¶€ ëœë¤ ë³€ê²½\n",
        "test_df_perturbed = test_df.copy()\n",
        "perturb_rate = 0.1  # 10%ë§Œ ì„ê¸°\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ì„ê¸°\n",
        "indices_to_perturb = random.sample(range(len(test_df_perturbed)), int(len(test_df_perturbed) * perturb_rate))\n",
        "test_df_perturbed.loc[indices_to_perturb, 'transcript'] = test_df_perturbed.loc[indices_to_perturb, 'transcript'].apply(shuffle_words)\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ ë³€í™˜ ë° íŒ¨ë”©\n",
        "X_perturbed = tokenizer.texts_to_sequences(test_df_perturbed['transcript'].astype(str).values)\n",
        "X_perturbed = pad_sequences(X_perturbed, maxlen=30, truncating='post', padding='post')\n",
        "\n",
        "# ì˜ˆì¸¡\n",
        "perturbed_predictions = model.predict(X_perturbed)\n",
        "test_df_perturbed['prediction'] = perturbed_predictions\n",
        "\n",
        "# í†µí™”ë³„ í‰ê·  ì˜ˆì¸¡ê°’ ê³„ì‚°\n",
        "perturbed_call_avg = test_df_perturbed.groupby('call_id')['prediction'].mean()\n",
        "perturbed_call_avg = perturbed_call_avg.apply(lambda x: 1 if x >= 0.5 else 0)\n",
        "\n",
        "# í‰ê°€\n",
        "true_labels = test_df_perturbed.groupby('call_id')['label'].first()\n",
        "pred_labels = perturbed_call_avg\n",
        "\n",
        "accuracy = accuracy_score(true_labels, pred_labels)\n",
        "precision = precision_score(true_labels, pred_labels)\n",
        "recall = recall_score(true_labels, pred_labels)\n",
        "f1 = f1_score(true_labels, pred_labels)\n",
        "\n",
        "print(\"\\nâœ… [í…ìŠ¤íŠ¸ ì„ê¸° í…ŒìŠ¤íŠ¸]\")\n",
        "print(f\"ì •í™•ë„: {accuracy:.4f}\")\n",
        "print(f\"ì •ë°€ë„: {precision:.4f}\")\n",
        "print(f\"ì¬í˜„ìœ¨: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "OdxqrFskZROd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "id": "SCDFbk1nbJrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 7ì°¨ lstmí•™ìŠµëª¨ë¸ ê·¸ë†ˆëª©ì†Œë¦¬ ìˆ˜ì‚¬ê¸°ê´€ ì‚¬ì¹­í˜• mp3íŒŒì¼ í…ŒìŠ¤íŠ¸ (ìŒì„± ì‹œê°„ ë¶„í•  ì—†ì´ í…ŒìŠ¤íŠ¸)\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "import whisper\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "\n",
        "# -------------------------------\n",
        "# 1. mp3 -> wav ë³€í™˜\n",
        "# -------------------------------\n",
        "def mp3_to_wav(mp3_file_path, wav_file_path):\n",
        "    audio = AudioSegment.from_mp3(mp3_file_path)\n",
        "    audio.export(wav_file_path, format=\"wav\")\n",
        "    print(f\"MP3 íŒŒì¼ì´ {wav_file_path}ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "    return wav_file_path\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Whisperë¡œ ìŒì„± ì¸ì‹ (STT)\n",
        "# -------------------------------\n",
        "def transcribe_audio_with_whisper(wav_file_path):\n",
        "    model = whisper.load_model(\"large\")  # Whisperì˜ ëª¨ë¸ ë¡œë“œ\n",
        "    result = model.transcribe(wav_file_path)\n",
        "    print(f\"ìŒì„± ì¸ì‹ ê²°ê³¼: {result['text']}\")\n",
        "    return result['text']\n",
        "\n",
        "# -------------------------------\n",
        "# 3. í…ìŠ¤íŠ¸ ì²˜ë¦¬ (Tokenizer ë¡œë”©, ì‹œí€€ìŠ¤ ë³€í™˜ ë° íŒ¨ë”©)\n",
        "# -------------------------------\n",
        "def preprocess_text(text, tokenizer, max_length=30):\n",
        "    sequences = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(sequences, maxlen=max_length, truncating='post', padding='post')\n",
        "    return padded_sequence\n",
        "\n",
        "# -------------------------------\n",
        "# 4. LSTM ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
        "# -------------------------------\n",
        "def predict_with_lstm(model, text, tokenizer):\n",
        "    processed_text = preprocess_text(text, tokenizer)\n",
        "    prediction = model.predict(processed_text)\n",
        "    return prediction\n",
        "\n",
        "# -------------------------------\n",
        "# 5. ëª¨ë¸ ë° Tokenizer ë¡œë”©\n",
        "# -------------------------------\n",
        "def load_model_and_tokenizer():\n",
        "    # LSTM ëª¨ë¸ê³¼ Tokenizer ë¡œë“œ\n",
        "    model = tf.keras.models.load_model('/content/drive/MyDrive/voicephishing1/lstm_model0514.keras')\n",
        "    with open('/content/drive/MyDrive/voicephishing1/tokenizer0514.pkl', 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    return model, tokenizer\n",
        "\n",
        "# -------------------------------\n",
        "# 6. MP3 íŒŒì¼ ê²½ë¡œ ì§€ì • í›„ ì²˜ë¦¬\n",
        "# -------------------------------\n",
        "def process_mp3(mp3_file_path):\n",
        "    # 1. mp3 íŒŒì¼ì„ wavë¡œ ë³€í™˜\n",
        "    wav_file_path = mp3_to_wav(mp3_file_path, 'temp_audio.wav')\n",
        "\n",
        "    # 2. Whisperë¡œ ìŒì„± ì¸ì‹ (STT)\n",
        "    transcribed_text = transcribe_audio_with_whisper(wav_file_path)\n",
        "\n",
        "    # 3. ëª¨ë¸ê³¼ Tokenizer ë¡œë“œ\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    # 4. ì˜ˆì¸¡\n",
        "    prediction = predict_with_lstm(model, transcribed_text, tokenizer)\n",
        "    print(f\"ì˜ˆì¸¡ ê²°ê³¼ (í”¼ì‹± í†µí™” í™•ë¥ ): {prediction[0][0]:.4f}\")\n",
        "\n",
        "    # ê²°ê³¼ ë°˜í™˜\n",
        "    return transcribed_text, prediction\n",
        "\n",
        "# -------------------------------\n",
        "# 7. ì˜ˆì‹œ: mp3 íŒŒì¼ ì²˜ë¦¬\n",
        "# -------------------------------\n",
        "mp3_file_path = '/content/drive/MyDrive/voicephishing1/example.mp3'  # ì²˜ë¦¬í•  mp3 íŒŒì¼ ê²½ë¡œ\n",
        "transcribed_text, prediction = process_mp3(mp3_file_path)\n",
        "print(f\"Transcribed Text: {transcribed_text}\")\n",
        "print(f\"Prediction: {prediction[0][0]:.4f}\")  # ì˜ˆì¸¡ í™•ë¥  ì¶œë ¥\n"
      ],
      "metadata": {
        "id": "SLMb_aY1Ze3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 7ì°¨ lstmí•™ìŠµëª¨ë¸ ê·¸ë†ˆëª©ì†Œë¦¬ ìˆ˜ì‚¬ê¸°ê´€ ì‚¬ì¹­í˜• mp3íŒŒì¼ í…ŒìŠ¤íŠ¸ (ìŒì„± ì‹œê°„ ë¶„í•  30ì´ˆ ì„¤ì • í…ŒìŠ¤íŠ¸)\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pydub import AudioSegment\n",
        "import whisper\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "# -------------------------------\n",
        "# 1. mp3 -> wav ë³€í™˜\n",
        "# -------------------------------\n",
        "def mp3_to_wav(mp3_file_path, wav_file_path):\n",
        "    audio = AudioSegment.from_mp3(mp3_file_path)\n",
        "    audio.export(wav_file_path, format=\"wav\")\n",
        "    print(f\"MP3 íŒŒì¼ì´ {wav_file_path}ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "    return wav_file_path\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Whisperë¡œ ìŒì„± ì¸ì‹ (STT)\n",
        "# -------------------------------\n",
        "def transcribe_audio_with_whisper(wav_file_path, start_ms, duration_ms):\n",
        "    model = whisper.load_model(\"large\")  # Whisperì˜ ëª¨ë¸ ë¡œë“œ\n",
        "    audio = AudioSegment.from_wav(wav_file_path)\n",
        "    segment = audio[start_ms:start_ms + duration_ms]  # êµ¬ê°„ë³„ë¡œ ìë¥´ê¸°\n",
        "    segment.export(\"temp_segment.wav\", format=\"wav\")\n",
        "\n",
        "    result = model.transcribe(\"temp_segment.wav\")\n",
        "    os.remove(\"temp_segment.wav\")  # ì„ì‹œ íŒŒì¼ ì‚­ì œ\n",
        "    print(f\"ìŒì„± ì¸ì‹ ê²°ê³¼ (êµ¬ê°„ {start_ms / 1000}s - {(start_ms + duration_ms) / 1000}s): {result['text']}\")\n",
        "    return result['text']\n",
        "\n",
        "# -------------------------------\n",
        "# 3. í…ìŠ¤íŠ¸ ì²˜ë¦¬ (Tokenizer ë¡œë”©, ì‹œí€€ìŠ¤ ë³€í™˜ ë° íŒ¨ë”©)\n",
        "# -------------------------------\n",
        "def preprocess_text(text, tokenizer, max_length=30):\n",
        "    sequences = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(sequences, maxlen=max_length, truncating='post', padding='post')\n",
        "    return padded_sequence\n",
        "\n",
        "# -------------------------------\n",
        "# 4. LSTM ëª¨ë¸ë¡œ ì˜ˆì¸¡\n",
        "# -------------------------------\n",
        "def predict_with_lstm(model, text, tokenizer):\n",
        "    processed_text = preprocess_text(text, tokenizer)\n",
        "    prediction = model.predict(processed_text)\n",
        "    return prediction\n",
        "\n",
        "# -------------------------------\n",
        "# 5. ëª¨ë¸ ë° Tokenizer ë¡œë”©\n",
        "# -------------------------------\n",
        "def load_model_and_tokenizer():\n",
        "    # LSTM ëª¨ë¸ê³¼ Tokenizer ë¡œë“œ\n",
        "    model = tf.keras.models.load_model('/content/drive/MyDrive/voicephishing1/lstm_model0514.keras')\n",
        "    with open('/content/drive/MyDrive/voicephishing1/tokenizer0514.pkl', 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    return model, tokenizer\n",
        "\n",
        "# -------------------------------\n",
        "# 6. MP3 íŒŒì¼ ê²½ë¡œ ì§€ì • í›„ ì²˜ë¦¬\n",
        "# -------------------------------\n",
        "def process_mp3(mp3_file_path, segment_duration_ms=20000):  # 20ì´ˆ êµ¬ê°„ìœ¼ë¡œ ë³€ê²½\n",
        "    # 1. mp3 íŒŒì¼ì„ wavë¡œ ë³€í™˜\n",
        "    wav_file_path = mp3_to_wav(mp3_file_path, 'temp_audio.wav')\n",
        "\n",
        "    # 2. ëª¨ë¸ê³¼ Tokenizer ë¡œë“œ\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    # 3. ìŒì„± íŒŒì¼ì˜ ì´ ê¸¸ì´ (ë°€ë¦¬ì´ˆ ë‹¨ìœ„)\n",
        "    audio = AudioSegment.from_wav(wav_file_path)\n",
        "    total_length_ms = len(audio)\n",
        "\n",
        "    # 4. ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
        "    time_intervals = []\n",
        "    predictions = []\n",
        "\n",
        "    # 5. ê° êµ¬ê°„ì— ëŒ€í•´ Whisperë¡œ ìŒì„± ì¸ì‹ í›„ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "    for start_ms in range(0, total_length_ms, segment_duration_ms):\n",
        "        transcribed_text = transcribe_audio_with_whisper(wav_file_path, start_ms, segment_duration_ms)\n",
        "        prediction = predict_with_lstm(model, transcribed_text, tokenizer)\n",
        "        time_intervals.append(start_ms / 1000)  # ì‹œê°„ (ì´ˆ ë‹¨ìœ„)\n",
        "        predictions.append(prediction[0][0])  # ì˜ˆì¸¡ í™•ë¥  ì €ì¥\n",
        "\n",
        "    # ê²°ê³¼ ë°˜í™˜\n",
        "    return time_intervals, predictions\n",
        "\n",
        "# -------------------------------\n",
        "# 7. ì˜ˆì‹œ: mp3 íŒŒì¼ ì²˜ë¦¬ ë° ì˜ˆì¸¡ ê²°ê³¼ ê·¸ë˜í”„ ì¶œë ¥\n",
        "# -------------------------------\n",
        "mp3_file_path = '/content/drive/MyDrive/voicephishing1/example.mp3'  # ì²˜ë¦¬í•  mp3 íŒŒì¼ ê²½ë¡œ\n",
        "time_intervals, predictions = process_mp3(mp3_file_path)\n",
        "\n",
        "# ê·¸ë˜í”„ ì‹œê°í™”\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(time_intervals, predictions, marker='o', color='b', linestyle='-', label='í”¼ì‹± í†µí™” í™•ë¥ ')\n",
        "plt.title(\"ì‹œê°„ì— ë”°ë¥¸ í”¼ì‹± í†µí™” í™•ë¥ \")\n",
        "plt.xlabel(\"ì‹œê°„ (ì´ˆ)\")\n",
        "plt.ylabel(\"í”¼ì‹± í†µí™” í™•ë¥ \")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hAu3Fy5bZhe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "import whisper\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "\n",
        "# -------------------------------\n",
        "# 1. mp3 â†’ wav ë³€í™˜\n",
        "# -------------------------------\n",
        "def mp3_to_wav(mp3_file_path, wav_file_path):\n",
        "    audio = AudioSegment.from_mp3(mp3_file_path)\n",
        "    audio.export(wav_file_path, format=\"wav\")\n",
        "    print(f\"âœ… MP3 â†’ WAV ë³€í™˜ ì™„ë£Œ: {wav_file_path}\")\n",
        "    return wav_file_path\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Whisper ìŒì„± ì¸ì‹\n",
        "# -------------------------------\n",
        "def transcribe_audio_with_whisper(wav_file_path):\n",
        "    model = whisper.load_model(\"large\")\n",
        "    result = model.transcribe(wav_file_path)\n",
        "    print(\"\\nğŸ“ ìŒì„± ì¸ì‹ ê²°ê³¼:\")\n",
        "    print(result['text'])\n",
        "    return result['text']\n",
        "\n",
        "# -------------------------------\n",
        "# 3. ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "# -------------------------------\n",
        "def preprocess_text(text, tokenizer, max_length=30):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # ë¬¸ì¥ë¶€í˜¸ ì œê±°\n",
        "    sequences = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "    return padded\n",
        "\n",
        "# -------------------------------\n",
        "# 4. ì˜ˆì¸¡ í•¨ìˆ˜\n",
        "# -------------------------------\n",
        "def predict_with_lstm(model, text, tokenizer):\n",
        "    padded_text = preprocess_text(text, tokenizer)\n",
        "    prediction = model.predict(padded_text, verbose=0)\n",
        "    return prediction[0][0]\n",
        "\n",
        "# -------------------------------\n",
        "# 5. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë”©\n",
        "# -------------------------------\n",
        "def load_model_and_tokenizer():\n",
        "    model = tf.keras.models.load_model('/content/drive/MyDrive/voicephishing1/lstm_model0514.keras')\n",
        "    with open('/content/drive/MyDrive/voicephishing1/tokenizer0514.pkl', 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    return model, tokenizer\n",
        "\n",
        "# -------------------------------\n",
        "# 6. ëˆ„ì  ìœ„í—˜ë„ ê³„ì‚° ë° ì¶œë ¥\n",
        "# -------------------------------\n",
        "def run_prediction_with_risk(text, model, tokenizer, base_threshold=0.5, detection_threshold=3.0):\n",
        "    print(\"\\nğŸ“Š ëˆ„ì  ìœ„í—˜ë„ ë¶„ì„ ì‹œì‘\")\n",
        "    lines = [line.strip() for line in text.split('.') if line.strip()]  # ë¬¸ì¥ ë¶„í• \n",
        "    cumulative_risk = 0.0\n",
        "    phishing_detected_at = None\n",
        "    predictions = []\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        prob = predict_with_lstm(model, line, tokenizer)\n",
        "        predictions.append(prob)\n",
        "\n",
        "        if prob > base_threshold:\n",
        "            cumulative_risk += (prob - base_threshold)\n",
        "        else:\n",
        "            cumulative_risk = max(0, cumulative_risk - (base_threshold - prob))\n",
        "\n",
        "        print(f\"{i+1:02d}. \\\"{line}\\\" â†’ í™•ë¥ : {prob:.4f}, ëˆ„ì  ìœ„í—˜ë„: {cumulative_risk:.4f}\")\n",
        "\n",
        "        if phishing_detected_at is None and cumulative_risk >= detection_threshold:\n",
        "            phishing_detected_at = i * 10  # ì‹œê°„ ê¸°ì¤€ 10ì´ˆ ë‹¨ìœ„ë¡œ ê°€ì •\n",
        "\n",
        "    if phishing_detected_at is not None:\n",
        "        print(f\"\\nâš ï¸ ë³´ì´ìŠ¤í”¼ì‹± ì˜ì‹¬ íƒì§€ ì‹œì : {phishing_detected_at}ì´ˆ\")\n",
        "    else:\n",
        "        print(\"\\nâœ… ë³´ì´ìŠ¤í”¼ì‹± ì˜ì‹¬ ì—†ìŒ\")\n",
        "\n",
        "    return lines, predictions, phishing_detected_at\n",
        "\n",
        "# -------------------------------\n",
        "# 7. ì‹œê°í™”\n",
        "# -------------------------------\n",
        "def plot_predictions(lines, predictions, phishing_detected_at=None):\n",
        "    time_intervals = [i * 10 for i in range(len(lines))]\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(time_intervals, predictions, marker='o', linestyle='-', color='red', label='ì˜ˆì¸¡ í™•ë¥ ')\n",
        "    plt.title(\"í”¼ì‹± í†µí™” í™•ë¥  ì‹œê°í™”\")\n",
        "    plt.xlabel(\"ì‹œê°„ (ì´ˆ)\")\n",
        "    plt.ylabel(\"í”¼ì‹± í†µí™” í™•ë¥ \")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(True)\n",
        "\n",
        "    if phishing_detected_at is not None:\n",
        "        plt.axvline(x=phishing_detected_at, color='blue', linestyle='--', label='íƒì§€ ì‹œì ')\n",
        "        plt.text(phishing_detected_at + 1, 0.85, 'âš  íƒì§€ë¨', color='blue')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 8. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
        "# -------------------------------\n",
        "def process_mp3(mp3_file_path):\n",
        "    # 1. mp3 â†’ wav\n",
        "    wav_file_path = mp3_to_wav(mp3_file_path, 'temp_audio.wav')\n",
        "\n",
        "    # 2. whisperë¡œ ìŒì„± ì¸ì‹\n",
        "    transcribed_text = transcribe_audio_with_whisper(wav_file_path)\n",
        "\n",
        "    # 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    # 4. ì˜ˆì¸¡ ë° ëˆ„ì  ìœ„í—˜ë„ ë¶„ì„\n",
        "    lines, predictions, phishing_detected_at = run_prediction_with_risk(transcribed_text, model, tokenizer)\n",
        "\n",
        "    # 5. ì‹œê°í™”\n",
        "    plot_predictions(lines, predictions, phishing_detected_at)\n",
        "\n",
        "    return transcribed_text, predictions\n",
        "\n",
        "# -------------------------------\n",
        "# 9. ì‹¤í–‰\n",
        "# -------------------------------\n",
        "mp3_file_path = '/content/drive/MyDrive/voicephishing1/example.mp3'\n",
        "transcribed_text, predictions = process_mp3(mp3_file_path)\n"
      ],
      "metadata": {
        "id": "gxxrSItAZkxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "import whisper\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "\n",
        "# -------------------------------\n",
        "# 1. mp3 â†’ wav ë³€í™˜\n",
        "# -------------------------------\n",
        "def mp3_to_wav(mp3_file_path, wav_file_path):\n",
        "    audio = AudioSegment.from_mp3(mp3_file_path)\n",
        "    audio.export(wav_file_path, format=\"wav\")\n",
        "    print(f\"âœ… MP3 â†’ WAV ë³€í™˜ ì™„ë£Œ: {wav_file_path}\")\n",
        "    return wav_file_path\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Whisper ìŒì„± ì¸ì‹\n",
        "# -------------------------------\n",
        "def transcribe_audio_with_whisper(wav_file_path):\n",
        "    model = whisper.load_model(\"large\")\n",
        "    result = model.transcribe(wav_file_path)\n",
        "    print(\"\\nğŸ“ ìŒì„± ì¸ì‹ ê²°ê³¼:\")\n",
        "    print(result['text'])\n",
        "    return result['text']\n",
        "\n",
        "# -------------------------------\n",
        "# 3. ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
        "# -------------------------------\n",
        "def preprocess_text(text, tokenizer, max_length=30):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # ë¬¸ì¥ë¶€í˜¸ ì œê±°\n",
        "    sequences = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "    return padded\n",
        "\n",
        "# -------------------------------\n",
        "# 4. ì˜ˆì¸¡ í•¨ìˆ˜\n",
        "# -------------------------------\n",
        "def predict_with_lstm(model, text, tokenizer):\n",
        "    padded_text = preprocess_text(text, tokenizer)\n",
        "    prediction = model.predict(padded_text, verbose=0)\n",
        "    return prediction[0][0]\n",
        "\n",
        "# -------------------------------\n",
        "# 5. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë”©\n",
        "# -------------------------------\n",
        "def load_model_and_tokenizer():\n",
        "    model = tf.keras.models.load_model('/content/drive/MyDrive/voicephishing1/lstm_model0514.keras')\n",
        "    with open('/content/drive/MyDrive/voicephishing1/tokenizer0514.pkl', 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 6. ëˆ„ì  í‰ê·  ê¸°ë°˜ ìœ„í—˜ë„ ë¶„ì„\n",
        "# -------------------------------\n",
        "def run_prediction_with_cumulative_average(text, model, tokenizer, threshold=0.7):\n",
        "    print(\"\\nğŸ“Š ëˆ„ì  í‰ê·  ìœ„í—˜ë„ ë¶„ì„ ì‹œì‘\")\n",
        "    lines = [line.strip() for line in text.split('.') if line.strip()]  # ë¬¸ì¥ ë¶„í• \n",
        "    predictions = []\n",
        "    cumulative_sum = 0.0\n",
        "    phishing_detected_at = None\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        prob = predict_with_lstm(model, line, tokenizer)\n",
        "        predictions.append(prob)\n",
        "        cumulative_sum += prob\n",
        "        cumulative_avg = cumulative_sum / (i + 1)\n",
        "\n",
        "        print(f\"{i+1:02d}. \\\"{line}\\\" â†’ í™•ë¥ : {prob:.4f}, ëˆ„ì  í‰ê· : {cumulative_avg:.4f}\")\n",
        "\n",
        "        if phishing_detected_at is None and cumulative_avg > threshold:\n",
        "            phishing_detected_at = i * 10  # ì‹œê°„ ë‹¨ìœ„ ê°€ì •\n",
        "\n",
        "    if phishing_detected_at is not None:\n",
        "        print(f\"\\nâš ï¸ ë³´ì´ìŠ¤í”¼ì‹± ì˜ì‹¬ íƒì§€ ì‹œì : {phishing_detected_at}ì´ˆ (ëˆ„ì  í‰ê·  ê¸°ë°˜)\")\n",
        "    else:\n",
        "        print(\"\\nâœ… ë³´ì´ìŠ¤í”¼ì‹± ì˜ì‹¬ ì—†ìŒ\")\n",
        "\n",
        "    return lines, predictions, phishing_detected_at\n",
        "\n",
        "# -------------------------------\n",
        "# 7. ì‹œê°í™” (ë³€ê²½ ì—†ìŒ)\n",
        "# -------------------------------\n",
        "def plot_predictions(lines, predictions, phishing_detected_at=None):\n",
        "    time_intervals = [i * 10 for i in range(len(lines))]\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(time_intervals, predictions, marker='o', linestyle='-', color='red', label='ì˜ˆì¸¡ í™•ë¥ ')\n",
        "    plt.title(\"í”¼ì‹± í†µí™” í™•ë¥  ì‹œê°í™”\")\n",
        "    plt.xlabel(\"ì‹œê°„ (ì´ˆ)\")\n",
        "    plt.ylabel(\"í”¼ì‹± í†µí™” í™•ë¥ \")\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(True)\n",
        "\n",
        "    if phishing_detected_at is not None:\n",
        "        plt.axvline(x=phishing_detected_at, color='blue', linestyle='--', label='íƒì§€ ì‹œì ')\n",
        "        plt.text(phishing_detected_at + 1, 0.85, 'âš  íƒì§€ë¨', color='blue')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# -------------------------------\n",
        "# 8. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
        "# -------------------------------\n",
        "def process_mp3(mp3_file_path):\n",
        "    # 1. mp3 â†’ wav\n",
        "    wav_file_path = mp3_to_wav(mp3_file_path, 'temp_audio.wav')\n",
        "\n",
        "    # 2. whisperë¡œ ìŒì„± ì¸ì‹\n",
        "    transcribed_text = transcribe_audio_with_whisper(wav_file_path)\n",
        "\n",
        "    # 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    # 4. ì˜ˆì¸¡ ë° ëˆ„ì  í‰ê·  ê¸°ë°˜ ë¶„ì„\n",
        "    lines, predictions, phishing_detected_at = run_prediction_with_cumulative_average(\n",
        "        transcribed_text, model, tokenizer\n",
        "    )\n",
        "\n",
        "    # 5. ì‹œê°í™”\n",
        "    plot_predictions(lines, predictions, phishing_detected_at)\n",
        "\n",
        "    return transcribed_text, predictions\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 9. ì‹¤í–‰\n",
        "# -------------------------------\n",
        "mp3_file_path = '/content/drive/MyDrive/voicephishing1/example.mp3'\n",
        "transcribed_text, predictions = process_mp3(mp3_file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "bCLI4AqhZsPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "import string\n",
        "\n",
        "# -------------------------------\n",
        "# 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "# -------------------------------\n",
        "def preprocess_text(text, tokenizer, max_length=30):\n",
        "    sequences = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(sequences, maxlen=max_length, truncating='post', padding='post')\n",
        "    return padded_sequence\n",
        "\n",
        "# -------------------------------\n",
        "# 2. LSTM ì˜ˆì¸¡\n",
        "# -------------------------------\n",
        "def predict_with_lstm(model, text, tokenizer):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # ë¬¸ì¥ë¶€í˜¸ ì œê±°\n",
        "    processed_text = preprocess_text(text, tokenizer)\n",
        "    prediction = model.predict(processed_text)\n",
        "    return prediction\n",
        "\n",
        "# -------------------------------\n",
        "# 3. ëª¨ë¸ ë° Tokenizer ë¡œë”©\n",
        "# -------------------------------\n",
        "def load_model_and_tokenizer():\n",
        "    model = tf.keras.models.load_model('/content/drive/MyDrive/voicephishing/lstm_model0514.keras')\n",
        "    with open('/content/drive/MyDrive/voicephishing/tokenizer0514.pkl', 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    return model, tokenizer\n",
        "\n",
        "# -------------------------------\n",
        "# 4. ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ê¸°ë°˜ ì²˜ë¦¬\n",
        "# -------------------------------\n",
        "def process_script(script_lines):\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "    time_intervals = []\n",
        "    predictions = []\n",
        "\n",
        "    for i, line in enumerate(script_lines):\n",
        "        prediction = predict_with_lstm(model, line, tokenizer)\n",
        "        time_intervals.append(i * 10)  # 10ì´ˆ ê°„ê²© ê¸°ì¤€\n",
        "        predictions.append(prediction[0][0])\n",
        "        print(f\"{i+1:02d}. \\\"{line}\\\" â†’ ì˜ˆì¸¡ í™•ë¥ : {prediction[0][0]:.4f}\")\n",
        "\n",
        "    return time_intervals, predictions\n",
        "\n",
        "# -------------------------------\n",
        "# 5. í…ŒìŠ¤íŠ¸í•  ë³´ì´ìŠ¤í”¼ì‹±ì¸ ì²™í•˜ëŠ” ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸\n",
        "# -------------------------------\n",
        "script_lines = [\n",
        "    \"ì—¬ë³´ì„¸ìš”\",  # í”¼í•´ì\n",
        "    \"ê³ ê°ë‹˜ ë§ìœ¼ì‹œì£ \",  # í”¼ì‹±ë²”\n",
        "    \"ë„¤, ëˆ„êµ¬ì„¸ìš”?\",  # í”¼í•´ì\n",
        "    \"ì§€ê¸ˆ ì ê¹ í†µí™” ê°€ëŠ¥í•˜ì‹ ê°€ìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ë„¤, ë§ì”€í•˜ì„¸ìš”\",  # í”¼í•´ì\n",
        "    \"ì €ëŠ” ê¸ˆìœµë³´í˜¸ì„¼í„° ìƒë‹´íŒ€ì— ìˆëŠ” ê¹€ìˆ˜í˜„ì´ë¼ê³  í•©ë‹ˆë‹¤\",  # í”¼ì‹±ë²”\n",
        "    \"ê¸ˆìœµë³´í˜¸ì„¼í„°ìš”?\",  # í”¼í•´ì\n",
        "    \"ë‹¤ë¦„ì´ ì•„ë‹ˆë¼ ê³ ê°ë‹˜ ëª…ì˜ë¡œ ê°œì„¤ëœ ê³„ì¢Œì—ì„œ ìµœê·¼ ì´ìƒ ê±°ë˜ê°€ ê°ì§€ë¼ì„œìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ì´ìƒ ê±°ë˜ìš”?\",  # í”¼í•´ì\n",
        "    \"í˜¹ì‹œ ì˜¤ëŠ˜ ì˜¤ì „ì— íœ´ëŒ€í°ìœ¼ë¡œ ì´ì²´í•˜ì‹  ì  ìˆìœ¼ì‹ ê°€ìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ì•„ë‡¨ ê·¸ëŸ° ì  ì—†ì–´ìš”\",  # í”¼í•´ì\n",
        "    \"í™•ì¸í•´ë³´ë‹ˆê¹Œ ê³ ê°ë‹˜ ê³„ì¢Œì—ì„œ ë¶€ì‚°ì—ì„œ ì ‘ì†í•œ ê¸°ë¡ì´ ë– ì„œìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ì € ë¶€ì‚°ì— ì—†ëŠ”ë°ìš”\",  # í”¼í•´ì\n",
        "    \"ê·¸ëŸ¼ ë³¸ì¸ í™•ì¸ë§Œ ê°„ë‹¨í•˜ê²Œ ë„ì™€ë“œë¦´ê²Œìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ì–´ë–»ê²Œ í•´ìš”?\",  # í”¼í•´ì\n",
        "    \"ì€í–‰ì€ êµ­ë¯¼ì€í–‰ ì‚¬ìš©í•˜ê³  ê³„ì‹œì£ \",  # í”¼ì‹±ë²”\n",
        "    \"ë„¤\",  # í”¼í•´ì\n",
        "    \"ë‹¤ë¥¸ ê±´ í•„ìš” ì—†ê³ ìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ì´ë¦„ì´ë‘ ê³„ì¢Œë²ˆí˜¸ ì• ë„¤ìë¦¬ë§Œ ë§ì”€í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤\",  # í”¼ì‹±ë²”\n",
        "    \"í™ê¸¸ë™, 1234ìš”\",  # í”¼í•´ì (ì˜ˆì‹œ)\n",
        "    \"ë„¤ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤\",  # í”¼ì‹±ë²”\n",
        "    \"ì§€ê¸ˆ ë°”ë¡œ ë³´í˜¸ì¡°ì¹˜ ë“¤ì–´ê°€ê³  ìˆìœ¼ë‹ˆê¹Œìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ì ì‹œ í›„ì— ì¸ì¦ë²ˆí˜¸ê°€ ë¬¸ìë¡œ ê°ˆ í…ë°\",  # í”¼ì‹±ë²”\n",
        "    \"ë°›ìœ¼ì‹œëŠ” ëŒ€ë¡œ ê·¸ ë²ˆí˜¸ë§Œ ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ì•„ ë„¤ ì•Œê² ìŠµë‹ˆë‹¤\",  # í”¼í•´ì\n",
        "    \"ì ˆì°¨ ëë‚˜ë©´ ìë™ìœ¼ë¡œ ë³´í˜¸ì¡°ì¹˜ ì™„ë£Œë©ë‹ˆë‹¤\",  # í”¼ì‹±ë²”\n",
        "    \"ê°ì‚¬í•©ë‹ˆë‹¤\",  # í”¼í•´ì\n",
        "    \"ë¶ˆí¸ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤\",  # í”¼ì‹±ë²”\n",
        "]\n",
        "\n",
        "# -------------------------------\n",
        "# 6. ì²˜ë¦¬ ë° ì‹œê°í™”\n",
        "# -------------------------------\n",
        "time_intervals, predictions = process_script(script_lines)\n",
        "\n",
        "# [ìˆ˜ì • 1] 0ì´ˆì—ì„œ 0ì„ ì¶”ê°€í•˜ì—¬ ê·¸ë˜í”„ê°€ 0,0ì—ì„œ ì‹œì‘í•˜ë„ë¡ ë§Œë“¦\n",
        "time_intervals = [0] + [t + 10 for t in time_intervals]\n",
        "predictions = [0] + predictions\n",
        "\n",
        "# [ìˆ˜ì • 2] xì¶• ëˆˆê¸ˆì„ 10ì´ˆ ë‹¨ìœ„ë¡œ ëª¨ë‘ í‘œì‹œ\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(time_intervals, predictions, marker='o', color='r', linestyle='-', label='í”¼ì‹± í†µí™” í™•ë¥ ')\n",
        "plt.title(\"ìŠ¤í¬ë¦½íŠ¸ì— ë”°ë¥¸ í”¼ì‹± í†µí™” í™•ë¥ \")\n",
        "plt.xlabel(\"ì‹œê°„ (ì´ˆ)\")\n",
        "plt.ylabel(\"í”¼ì‹± í†µí™” í™•ë¥ \")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.xticks(np.arange(0, max(time_intervals) + 10, 10))\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_JfuPGd9ZvI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "import string\n",
        "\n",
        "# -------------------------------\n",
        "# 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "# -------------------------------\n",
        "def preprocess_text(text, tokenizer, max_length=30):\n",
        "    sequences = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(sequences, maxlen=max_length, truncating='post', padding='post')\n",
        "    return padded_sequence\n",
        "\n",
        "# -------------------------------\n",
        "# 2. LSTM ì˜ˆì¸¡\n",
        "# -------------------------------\n",
        "def predict_with_lstm(model, text, tokenizer):\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # ë¬¸ì¥ë¶€í˜¸ ì œê±°\n",
        "    processed_text = preprocess_text(text, tokenizer)\n",
        "    prediction = model.predict(processed_text, verbose=0)\n",
        "    return prediction\n",
        "\n",
        "# -------------------------------\n",
        "# 3. ëª¨ë¸ ë° Tokenizer ë¡œë”©\n",
        "# -------------------------------\n",
        "def load_model_and_tokenizer():\n",
        "    model = tf.keras.models.load_model('/content/drive/MyDrive/voicephishing1/lstm_model0514.keras')\n",
        "    with open('/content/drive/MyDrive/voicephishing1/tokenizer0514.pkl', 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    return model, tokenizer\n",
        "\n",
        "# -------------------------------\n",
        "# 4. ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ê¸°ë°˜ ì²˜ë¦¬ (ëˆ„ì  ìœ„í—˜ë„ í¬í•¨)\n",
        "# -------------------------------\n",
        "def process_script(script_lines, base_threshold=0.5, detection_threshold=3.0):\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "    time_intervals = []\n",
        "    predictions = []\n",
        "    cumulative_risk = 0.0\n",
        "    phishing_detected_at = None\n",
        "\n",
        "    print(\"\\n[ì˜ˆì¸¡ ê²°ê³¼ ë° ëˆ„ì  ìœ„í—˜ë„]\\n\")\n",
        "    for i, line in enumerate(script_lines):\n",
        "        prediction = predict_with_lstm(model, line, tokenizer)\n",
        "        prob = prediction[0][0]\n",
        "        predictions.append(prob)\n",
        "        time_intervals.append(i * 10)\n",
        "\n",
        "        # ëˆ„ì  ìœ„í—˜ë„ ê³„ì‚°\n",
        "        if prob > base_threshold:\n",
        "            cumulative_risk += (prob - base_threshold)\n",
        "        else:\n",
        "            cumulative_risk = max(0, cumulative_risk - (base_threshold - prob))\n",
        "\n",
        "        print(f\"{i+1:02d}. \\\"{line}\\\" â†’ í™•ë¥ : {prob:.4f}, ëˆ„ì  ìœ„í—˜ë„: {cumulative_risk:.4f}\")\n",
        "\n",
        "        if phishing_detected_at is None and cumulative_risk >= detection_threshold:\n",
        "            phishing_detected_at = i * 10\n",
        "\n",
        "    if phishing_detected_at is not None:\n",
        "        print(f\"\\nâš ï¸ ë³´ì´ìŠ¤í”¼ì‹± ì˜ì‹¬ íƒì§€ ì‹œì : {phishing_detected_at}ì´ˆ\")\n",
        "    else:\n",
        "        print(\"\\nâœ… ë³´ì´ìŠ¤í”¼ì‹± ì˜ì‹¬ë˜ì§€ ì•ŠìŒ\")\n",
        "\n",
        "    return time_intervals, predictions, phishing_detected_at\n",
        "\n",
        "# -------------------------------\n",
        "# 5. í…ŒìŠ¤íŠ¸í•  ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸\n",
        "# -------------------------------\n",
        "script_lines = [\n",
        "    \"ì—¬ë³´ì„¸ìš”\",  # í”¼í•´ì\n",
        "    \"ê³ ê°ë‹˜ ë§ìœ¼ì‹œì£ \",  # í”¼ì‹±ë²”\n",
        "    \"ë„¤, ëˆ„êµ¬ì„¸ìš”?\",  # í”¼í•´ì\n",
        "    \"ì§€ê¸ˆ ì ê¹ í†µí™” ê°€ëŠ¥í•˜ì‹ ê°€ìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ë„¤, ë§ì”€í•˜ì„¸ìš”\",  # í”¼í•´ì\n",
        "    \"ì €ëŠ” ê¸ˆìœµë³´í˜¸ì„¼í„° ìƒë‹´íŒ€ì— ìˆëŠ” ê¹€ìˆ˜í˜„ì´ë¼ê³  í•©ë‹ˆë‹¤\",  # í”¼ì‹±ë²”\n",
        "    \"ê¸ˆìœµë³´í˜¸ì„¼í„°ìš”?\",  # í”¼í•´ì\n",
        "    \"ë‹¤ë¦„ì´ ì•„ë‹ˆë¼ ê³ ê°ë‹˜ ëª…ì˜ë¡œ ê°œì„¤ëœ ê³„ì¢Œì—ì„œ ìµœê·¼ ì´ìƒ ê±°ë˜ê°€ ê°ì§€ë¼ì„œìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ì´ìƒ ê±°ë˜ìš”?\",  # í”¼í•´ì\n",
        "    \"í˜¹ì‹œ ì˜¤ëŠ˜ ì˜¤ì „ì— íœ´ëŒ€í°ìœ¼ë¡œ ì´ì²´í•˜ì‹  ì  ìˆìœ¼ì‹ ê°€ìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ì•„ë‡¨ ê·¸ëŸ° ì  ì—†ì–´ìš”\",  # í”¼í•´ì\n",
        "    \"í™•ì¸í•´ë³´ë‹ˆê¹Œ ê³ ê°ë‹˜ ê³„ì¢Œì—ì„œ ë¶€ì‚°ì—ì„œ ì ‘ì†í•œ ê¸°ë¡ì´ ë– ì„œìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ì € ë¶€ì‚°ì— ì—†ëŠ”ë°ìš”\",  # í”¼í•´ì\n",
        "    \"ê·¸ëŸ¼ ë³¸ì¸ í™•ì¸ë§Œ ê°„ë‹¨í•˜ê²Œ ë„ì™€ë“œë¦´ê²Œìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ì–´ë–»ê²Œ í•´ìš”?\",  # í”¼í•´ì\n",
        "    \"ì€í–‰ì€ êµ­ë¯¼ì€í–‰ ì‚¬ìš©í•˜ê³  ê³„ì‹œì£ \",  # í”¼ì‹±ë²”\n",
        "    \"ë„¤\",  # í”¼í•´ì\n",
        "    \"ë‹¤ë¥¸ ê±´ í•„ìš” ì—†ê³ ìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ì´ë¦„ì´ë‘ ê³„ì¢Œë²ˆí˜¸ ì• ë„¤ìë¦¬ë§Œ ë§ì”€í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤\",  # í”¼ì‹±ë²”\n",
        "    \"í™ê¸¸ë™, 1234ìš”\",  # í”¼í•´ì (ì˜ˆì‹œ)\n",
        "    \"ë„¤ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤\",  # í”¼ì‹±ë²”\n",
        "    \"ì§€ê¸ˆ ë°”ë¡œ ë³´í˜¸ì¡°ì¹˜ ë“¤ì–´ê°€ê³  ìˆìœ¼ë‹ˆê¹Œìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ì ì‹œ í›„ì— ì¸ì¦ë²ˆí˜¸ê°€ ë¬¸ìë¡œ ê°ˆ í…ë°\",  # í”¼ì‹±ë²”\n",
        "    \"ë°›ìœ¼ì‹œëŠ” ëŒ€ë¡œ ê·¸ ë²ˆí˜¸ë§Œ ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”\",  # í”¼ì‹±ë²”\n",
        "    \"ì•„ ë„¤ ì•Œê² ìŠµë‹ˆë‹¤\",  # í”¼í•´ì\n",
        "    \"ì ˆì°¨ ëë‚˜ë©´ ìë™ìœ¼ë¡œ ë³´í˜¸ì¡°ì¹˜ ì™„ë£Œë©ë‹ˆë‹¤\",  # í”¼ì‹±ë²”\n",
        "    \"ê°ì‚¬í•©ë‹ˆë‹¤\",  # í”¼í•´ì\n",
        "    \"ë¶ˆí¸ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤\",  # í”¼ì‹±ë²”\n",
        "]\n",
        "\n",
        "# -------------------------------\n",
        "# 6. ì²˜ë¦¬ ë° ì‹œê°í™”\n",
        "# -------------------------------\n",
        "time_intervals, predictions, phishing_detected_at = process_script(script_lines)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(time_intervals, predictions, marker='o', color='r', linestyle='-', label='í”¼ì‹± í†µí™” í™•ë¥ ')\n",
        "plt.title(\"ìŠ¤í¬ë¦½íŠ¸ì— ë”°ë¥¸ í”¼ì‹± í†µí™” í™•ë¥ \")\n",
        "plt.xlabel(\"ì‹œê°„ (ì´ˆ)\")\n",
        "plt.ylabel(\"í”¼ì‹± í†µí™” í™•ë¥ \")\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# íƒì§€ ì‹œì  ì‹œê°í™”\n",
        "if phishing_detected_at is not None:\n",
        "    plt.axvline(x=phishing_detected_at, color='blue', linestyle='--', label='íƒì§€ ì‹œì ')\n",
        "    plt.text(phishing_detected_at + 1, 0.9, 'âš  íƒì§€ë¨', color='blue')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1KLE2uZ-Z5sk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}